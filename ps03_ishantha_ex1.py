# -*- coding: utf-8 -*-
"""ps03_Ishantha_Ex1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HHtqVzb1yn-N0veKwLNp7N0o_WoiHMk3

Exercise 1: Basic Word Tokenization Using NLTK

This script demonstrates how to tokenize text using the Natural Language Toolkit (NLTK).
It involves splitting a paragraph of text into individual words (tokens),
counting the number of tokens, and analyzing the frequency of each token.

Step 1: Install the NLTK library
"""

!pip install nltk

"""Step 2: Import the necessary functions from NLTK"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
nltk.download('punkt')  # Download the necessary data

"""Step 4: Define a text paragraph
The text should contain at least 4 sentences about any topic.
"""

text = "The world of artificial intelligence is rapidly evolving, changing the way we live and work. From self-driving cars to personalized healthcare, AI technologies are making a significant impact on many aspects of our daily lives. As these advancements continue, ethical considerations and regulations are becoming more important to ensure that AI is used responsibly and fairly. Despite the challenges, the potential benefits of AI are vast, promising to revolutionize industries and improve quality of life for people around the globe."

"""Step 5: Tokenize the text into words using NLTK's word_tokenize function

Step 6: Display the list of tokens and count the number of tokens
"""

tokens = word_tokenize(text)
print("Tokens:", tokens)
print("Number of Tokens:", len(tokens))

"""Step 7: Analyze the frequency of each token using FreqDist"""

frequency = FreqDist(tokens)
print("Token Frequency:", frequency.most_common())